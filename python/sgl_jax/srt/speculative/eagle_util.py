from __future__ import annotations

import copy
import logging
import os
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, ClassVar

import jax
import jax.numpy as jnp
import numpy
import numpy as np
from flax import nnx
from jax.tree_util import register_pytree_node_class

from sgl_jax.srt.layers.logits_processor import LogitsProcessorOutput

if TYPE_CHECKING:
    from sgl_jax.srt.managers.scheduler import GenerationBatchResult

from sgl_jax.srt.managers.schedule_batch import (
    ModelWorkerBatch,
    ScheduleBatch,
    get_last_loc,
    global_server_args_dict,
)
from sgl_jax.srt.managers.tp_worker import ModelWorker
from sgl_jax.srt.mem_cache.allocator import BaseTokenToKVPoolAllocator
from sgl_jax.srt.mem_cache.common import (
    alloc_paged_token_slots_extend,
    alloc_token_slots,
)
from sgl_jax.srt.model_executor.forward_batch_info import (
    CaptureHiddenMode,
    ForwardBatch,
    ForwardMode,
)
from sgl_jax.srt.speculative.pallas.kernel import (
    create_extend_after_decode_spec_info,
    top_k_renorm_prob,
    top_p_renorm_prob,
    tree_speculative_sampling_target_only,
    verify_tree_greedy,
)

logger = logging.getLogger(__name__)

SIMULATE_ACC_LEN = os.environ.get("SIMULATE_ACC_LEN")
SIMULATE_ACC_METHOD = os.environ.get("SIMULATE_ACC_METHOD", "multinomial")


def _is_jax_leaf(value: Any) -> bool:
    """Detect sentinel nodes generated by jax.tree_util when shaping pytrees."""
    cls = value.__class__
    return cls.__name__ == "Leaf" and cls.__module__.startswith("jax.")


def _as_int32_array(value: Any, *, fallback: int = -1) -> jax.Array:
    """Convert scalar-like inputs into scalar int32 JAX arrays."""
    if isinstance(value, jax.Array):
        return value
    if isinstance(value, numpy.ndarray):
        return jnp.asarray(value, dtype=jnp.int32)
    if isinstance(value, (int, numpy.integer)):
        return jnp.asarray(int(value), dtype=jnp.int32)
    if isinstance(value, (list, tuple)):
        return jnp.asarray(value, dtype=jnp.int32)
    if _is_jax_leaf(value):
        return jnp.asarray(fallback, dtype=jnp.int32)
    try:
        return jnp.asarray(value, dtype=jnp.int32)
    except (TypeError, ValueError) as exc:
        raise TypeError(
            f"Unable to convert value of type {type(value)} into int32 metadata array."
        ) from exc


def get_last_loc_jax_array(
    req_to_token: jax.Array,
    req_pool_indices: jax.Array,
    prefix_lens: jax.Array,
) -> jax.Array:
    """JAX version of get_last_loc that operates on JAX arrays.

    Args:
        req_to_token: Token mapping tensor of shape (num_reqs, max_seq_len)
        req_pool_indices: Request pool indices of shape (batch_size,)
        prefix_lens: Prefix lengths of shape (batch_size,)

    Returns:
        Last location tensor of shape (batch_size,)
    """
    return jnp.where(
        prefix_lens > 0,
        req_to_token[req_pool_indices, prefix_lens - 1],
        jnp.full_like(prefix_lens, -1),
    )


def get_last_loc_large_page_size_top_k_1(
    req_to_token: jax.Array,
    req_pool_indices: jax.Array,
    seq_lens: jax.Array,
    speculative_num_steps: int,
) -> tuple[jax.Array, jax.Array, jax.Array]:
    """JAX implementation of get_last_loc_large_page_size_top_k_1.

    This function is used in EAGLE speculative decoding to compute cache locations
    for large page sizes when top_k=1.

    Args:
        req_to_token: Request to token mapping tensor
        req_pool_indices: Request pool indices
        seq_lens: Current sequence lengths
        speculative_num_steps: Number of speculative decoding steps

    Returns:
        tuple of (prefix_lens, new_seq_lens, last_loc):
        - prefix_lens: Same as input seq_lens
        - new_seq_lens: Updated sequence lengths (prefix_lens + speculative_num_steps)
        - last_loc: Last cache locations computed using get_last_loc
    """
    prefix_lens = seq_lens
    new_seq_lens = prefix_lens + speculative_num_steps
    last_loc = get_last_loc_jax_array(
        req_to_token,
        req_pool_indices,
        prefix_lens,
    )
    return prefix_lens, new_seq_lens, last_loc


def get_last_loc_large_page_size_large_top_k(
    req_to_token: jax.Array,
    req_pool_indices: jax.Array,
    seq_lens: jax.Array,
    speculative_num_steps: int,
    topk: int,
    page_size: int,
) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:
    """JAX implementation of get_last_loc_large_page_size_large_top_k.

    This function handles large page sizes with large top_k values in EAGLE speculative decoding.
    It computes cache locations and manages page allocation for multiple top-k branches.

    Args:
        req_to_token: Request to token mapping tensor
        req_pool_indices: Request pool indices
        seq_lens: Current sequence lengths
        speculative_num_steps: Number of speculative decoding steps
        topk: Number of top-k branches
        page_size: Size of each memory page

    Returns:
        tuple of (prefix_lens, new_seq_lens, last_loc, num_new_pages_per_topk, extend_lens):
        - prefix_lens: Same as input seq_lens
        - new_seq_lens: Updated sequence lengths considering page alignment
        - last_loc: Last cache locations
        - num_new_pages_per_topk: Number of new pages needed per top-k branch
        - extend_lens: Number of tokens to extend for each sequence
    """
    prefix_lens = seq_lens
    last_page_lens = prefix_lens % page_size
    num_new_pages_per_topk = (last_page_lens + speculative_num_steps + page_size - 1) // page_size

    new_seq_lens = prefix_lens // page_size * page_size + num_new_pages_per_topk * (
        page_size * topk
    )
    extend_lens = new_seq_lens - prefix_lens

    last_loc = get_last_loc_jax_array(
        req_to_token,
        req_pool_indices,
        prefix_lens,
    )

    return prefix_lens, new_seq_lens, last_loc, num_new_pages_per_topk, extend_lens


def build_tree_kernel_efficient_preprocess(
    verified_id: jax.Array,
    score_list: list[jax.Array],
    token_list: list[jax.Array],
    parents_list: list[jax.Array],
    num_verify_tokens: int,
):
    # Concatenate score_list along dim=1 and flatten from dim=1 onwards
    # b, n, topk; n = 1 + (num_steps-1) * self.topk
    score_tensor = jnp.concatenate(score_list, axis=1)
    score_tensor = score_tensor.reshape(score_tensor.shape[0], -1)

    # Concatenate token lists: b, (self.topk + (num_steps-1) * self.topk)
    ss_token_list = jnp.concatenate(token_list, axis=1)

    # Get top scores and indices
    _, top_scores_index = jax.lax.top_k(score_tensor, num_verify_tokens - 1)
    top_scores_index = jnp.sort(top_scores_index, axis=-1)

    # Gather draft tokens using the top indices
    draft_tokens = jnp.take_along_axis(ss_token_list, top_scores_index, axis=1)
    # assert draft_tokens.shape == (batch_size, verified_id.shape[0])
    draft_tokens = jnp.concatenate(
        [jnp.expand_dims(verified_id, axis=1), draft_tokens], axis=1
    ).flatten()

    # Build parent list
    if len(parents_list) > 1:
        parent_list = jnp.concatenate(parents_list[:-1], axis=1)
    else:
        batch_size = parents_list[0].shape[0]
        parent_list = jnp.empty((batch_size, 0), dtype=jnp.int32)

    return parent_list, top_scores_index, draft_tokens


def build_tree_kernel_efficient(
    verified_id: jax.Array,
    score_list: list[jax.Array],
    token_list: list[jax.Array],
    parents_list: list[jax.Array],
    seq_lens: jax.Array,
    seq_lens_sum: int,
    topk: int,
    spec_steps: int,
    num_verify_tokens: int,
    max_seq_len_per_req: int,
) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:
    """JAX implementation of build_tree_kernel_efficient.

    Args:
        verified_id: Verified token IDs from previous step
        score_list: List of score tensors from draft model
        token_list: List of token tensors from draft model
        parents_list: List of parent index tensors
        seq_lens: Sequence lengths
        seq_lens_sum: Sum of sequence lengths
        topk: Number of top-k candidates
        spec_steps: Number of speculative steps
        num_verify_tokens: Number of tokens to verify
        max_seq_len_per_req: Maximum allowed sequence length per request (static bound)

    Returns:
        tuple of (tree_mask, positions, retrive_index, retrive_next_token,
                 retrive_next_sibling, draft_tokens)
    """
    parent_list, top_scores_index, draft_tokens = build_tree_kernel_efficient_preprocess(
        verified_id, score_list, token_list, parents_list, num_verify_tokens
    )

    # Get batch size
    bs = seq_lens.shape[0]
    actual_tree_mask_size = (
        seq_lens_sum * num_verify_tokens + num_verify_tokens * num_verify_tokens * bs
    )
    max_tree_mask_size = (
        max_seq_len_per_req * num_verify_tokens * bs + num_verify_tokens * num_verify_tokens * bs
    )

    tree_mask, positions, retrive_index, retrive_next_token, retrive_next_sibling = (
        build_eagle_tree_structure(
            parent_list=parent_list,
            selected_index=top_scores_index,
            verified_seq_len=seq_lens,
            bs=bs,
            draft_token_num=num_verify_tokens,
            topk=topk,
            depth=spec_steps,
            seq_lens_sum=seq_lens_sum,
            tree_mask_mode=0,  # FULL_MASK
            max_seq_len_per_req=max_seq_len_per_req,
            max_tree_mask_size=max_tree_mask_size,
            actual_tree_mask_size=actual_tree_mask_size,
        )
    )

    return (
        tree_mask,
        positions,
        retrive_index,
        retrive_next_token,
        retrive_next_sibling,
        draft_tokens,
    )


@register_pytree_node_class
@dataclass
class EagleDraftInput:
    # Constant: alloc length per decode step
    ALLOC_LEN_PER_DECODE: ClassVar[int] = None

    # The inputs for decode
    # shape: (b, topk)
    topk_p: jax.Array = None
    topk_index: jax.Array = None
    # shape: (b, hidden_size)
    hidden_states: jax.Array = None
    capture_hidden_mode: CaptureHiddenMode = CaptureHiddenMode.FULL

    # Inputs for extend
    # shape: (b,)
    verified_id: jax.Array = None
    accept_length: jax.Array = None
    accept_length_cpu: jax.Array | None = None

    # Inputs for the attention backends
    # shape: (b + 1,)
    kv_indptr: jax.Array = None
    kv_indices: jax.Array = None

    # Shape info for padding
    num_tokens_per_batch: int = -1
    num_tokens_for_logprob_per_batch: int = -1

    # Inputs for draft extend
    # shape: (b,)
    seq_lens_for_draft_extend: jax.Array = None
    req_pool_indices_for_draft_extend: jax.Array = None
    # todo some fields is unuseful

    # Inputs for V2 overlap worker
    # future_indices: Optional[FutureIndices] = None
    allocate_lens: jax.Array | None = None
    new_seq_lens: jax.Array | None = None
    # verify_done: Optional[torch.cuda.Event] = None

    def tree_flatten(self):
        accept_length_cpu_arr = (
            jnp.empty((0,), dtype=jnp.int32)
            if self.accept_length_cpu is None
            else _as_int32_array(self.accept_length_cpu, fallback=0)
        )

        num_tokens_per_batch_arr = _as_int32_array(self.num_tokens_per_batch)
        num_tokens_for_logprob_arr = _as_int32_array(self.num_tokens_for_logprob_per_batch)

        children = (
            self.topk_p,
            self.topk_index,
            self.hidden_states,
            self.verified_id,
            self.accept_length,
            self.kv_indptr,
            self.kv_indices,
            self.seq_lens_for_draft_extend,
            self.req_pool_indices_for_draft_extend,
            accept_length_cpu_arr,
            num_tokens_per_batch_arr,
            num_tokens_for_logprob_arr,
        )

        aux_data = {
            "capture_hidden_mode": self.capture_hidden_mode,
        }
        return (children, aux_data)

    @classmethod
    def tree_unflatten(cls, aux_data, children):
        obj = cls.__new__(cls)
        obj.capture_hidden_mode = aux_data["capture_hidden_mode"]
        obj.topk_p = children[0]
        obj.topk_index = children[1]
        obj.hidden_states = children[2]
        obj.verified_id = children[3]
        obj.accept_length = children[4]
        obj.kv_indptr = children[5]
        obj.kv_indices = children[6]
        obj.seq_lens_for_draft_extend = children[7]
        obj.req_pool_indices_for_draft_extend = children[8]

        obj.accept_length_cpu = children[9]
        obj.num_tokens_per_batch = children[10]
        obj.num_tokens_for_logprob_per_batch = children[11]

        return obj

    def prepare_for_extend_after_target_prefill(self, model_worker_batch: ModelWorkerBatch):

        if model_worker_batch.forward_mode.is_idle():
            return

        # Prefill only generate 1 token.
        assert (
            self.verified_id.shape[0] == model_worker_batch.real_bs
        ), f"{self.verified_id.shape=} {model_worker_batch.real_bs=}"

        pt = 0
        for i in range(model_worker_batch.real_bs):
            extend_len = model_worker_batch.seq_lens[i]
            input_ids = model_worker_batch.input_ids[pt : pt + extend_len]
            # TODO: batch.input_ids should on tpu
            model_worker_batch.input_ids[pt : pt + extend_len - 1] = input_ids[1:]
            model_worker_batch.input_ids[pt + extend_len - 1] = self.verified_id[i]
            pt += extend_len

    def prepare_for_extend_after_verify(
        self,
        model_worker_batch: ModelWorkerBatch,
        predict: jax.Array,
        num_draft_tokens: int,
        draft_model_runner: Any,
        batch_output: GenerationBatchResult,
    ):
        seq_lens_cpu_ = model_worker_batch.seq_lens_cpu
        extend_num_tokens = len(model_worker_batch.seq_lens) * num_draft_tokens

        model_worker_batch.spec_info = self
        model_worker_batch.input_ids = predict
        model_worker_batch.seq_lens = model_worker_batch.seq_lens + num_draft_tokens
        model_worker_batch.seq_lens_cpu = model_worker_batch.seq_lens_cpu + num_draft_tokens
        model_worker_batch.seq_lens_sum += extend_num_tokens
        model_worker_batch.extend_seq_lens = [
            num_draft_tokens for _ in range(len(model_worker_batch.seq_lens))
        ]
        model_worker_batch.extend_prefix_lens = seq_lens_cpu_.tolist()
        model_worker_batch.extend_num_tokens = extend_num_tokens
        model_worker_batch.capture_hidden_mode = CaptureHiddenMode.FULL
        model_worker_batch.forward_mode = ForwardMode.DRAFT_EXTEND
        model_worker_batch.spec_info.hidden_states = batch_output.next_draft_input.hidden_states
        forward_batch = ForwardBatch.init_new(model_worker_batch, draft_model_runner)
        draft_model_runner.attn_backend.init_forward_metadata(forward_batch)
        return forward_batch

    def prepare_for_decode(self, schedule_batch: ScheduleBatch):
        new_allocate_lens = schedule_batch.seq_lens + self.ALLOC_LEN_PER_DECODE

        # TODO(pc) implement overlap here
        # schedule_batch.maybe_wait_verify_done()
        bs = schedule_batch.batch_size()

        page_size = schedule_batch.token_to_kv_pool_allocator.page_size

        if page_size == 1:
            num_needed_tokens = (new_allocate_lens - self.allocate_lens).sum().item()
            out_cache_loc = alloc_token_slots(schedule_batch.tree_cache, num_needed_tokens)
        else:
            last_loc = get_last_loc(
                schedule_batch.req_to_token_pool.req_to_token,
                schedule_batch.req_pool_indices,
                self.allocate_lens,
            )
            extend_num_tokens = sum(new_allocate_lens - self.allocate_lens).item()
            out_cache_loc = alloc_paged_token_slots_extend(
                schedule_batch.tree_cache,
                self.allocate_lens,
                self.allocate_lens,
                last_loc,
                extend_num_tokens,
            )

        assign_req_to_token_pool(
            schedule_batch.req_pool_indices,
            schedule_batch.req_to_token_pool.req_to_token,
            self.allocate_lens,
            new_allocate_lens,
            out_cache_loc,
        )
        self.allocate_lens = new_allocate_lens

        # FIXME(lsyin): make this sync optional
        # schedule_batch.seq_lens_cpu = schedule_batch.seq_lens
        schedule_batch.seq_lens_sum = np.sum(schedule_batch.seq_lens).item()

    def prepare_for_draft_decode(
        self, model_worker_batch: ModelWorkerBatch, topk: int, num_steps: int
    ):
        self.capture_hidden_mode = CaptureHiddenMode.LAST
        self.num_tokens_per_batch = topk
        self.num_tokens_for_logprob_per_batch = topk
        model_worker_batch.return_hidden_states = False
        bs = model_worker_batch.seq_lens.shape[0]
        # we don't need process outcache_loc because we don't need this in attention backend
        # out_cache_loc = np.empty((bs * topk * num_steps), dtype=np.int32)
        # model_worker_batch.out_cache_loc = out_cache_loc
        model_worker_batch.seq_lens_sum = int(jnp.sum(model_worker_batch.seq_lens))
        model_worker_batch.return_hidden_states = False
        model_worker_batch.spec_info.positions = jnp.repeat(model_worker_batch.seq_lens, topk)

    @classmethod
    def create_idle_input(
        cls,
        hidden_size: int,
        dtype: jnp.dtype,
        topk: int,
        capture_hidden_mode: CaptureHiddenMode,
    ):
        return cls(
            verified_id=jnp.empty((0,), dtype=jnp.int32),
            hidden_states=jnp.empty((0, hidden_size), dtype=dtype),
            topk_p=jnp.empty((0, topk), dtype=jnp.float32),
            topk_index=jnp.empty((0, topk), dtype=jnp.int32),
            capture_hidden_mode=capture_hidden_mode,
            accept_length=jnp.empty((0,), dtype=jnp.int32),
            accept_length_cpu=jnp.empty((0,), dtype=jnp.int32),
        )

    @DeprecationWarning
    def prepare_extend_after_decode(
        self,
        batch: ScheduleBatch,
    ):
        if batch.forward_mode.is_idle():
            return

        batch.input_ids = self.verified_id
        accept_length_cpu_arr = batch.spec_info.accept_length_cpu
        if accept_length_cpu_arr is None:
            accept_length_cpu_host = numpy.asarray([], dtype=numpy.int32)
        else:
            accept_length_cpu_host = numpy.asarray(
                jax.device_get(accept_length_cpu_arr), dtype=numpy.int32
            )
        batch.extend_lens = (accept_length_cpu_host + 1).tolist()
        batch.extend_num_tokens = sum(batch.extend_lens)
        batch.seq_lens = batch.spec_info.seq_lens_for_draft_extend
        batch.seq_lens_sum = batch.seq_lens.sum().item()
        batch.req_pool_indices = batch.spec_info.req_pool_indices_for_draft_extend
        batch.return_logprob = False
        batch.return_hidden_states = False

        self.capture_hidden_mode = CaptureHiddenMode.LAST
        self.accept_length = self.accept_length + 1
        self.positions = jnp.empty_like(batch.input_ids, dtype=jnp.int32)
        self.verified_id = jnp.empty_like(self.accept_length, dtype=jnp.int32)

        self.positions, self.verified_id = create_extend_after_decode_spec_info(
            batch.input_ids,
            batch.seq_lens,
            self.accept_length,
            self.positions,
            self.verified_id,
        )

        self.accept_length_cpu = jnp.asarray(accept_length_cpu_host, dtype=jnp.int32)

    def generate_attn_arg_prefill(
        self,
        req_pool_indices: jax.Array,
        paged_kernel_lens: jax.Array,
        paged_kernel_lens_sum: int,
        req_to_token: jax.Array,
    ):
        pass

    def filter_batch(self, new_indices: jax.Array, has_been_filtered: bool = True):
        # FIXME(pc) need support overlap here
        if has_been_filtered:
            # in eagle_utils.py:verify, we have already filtered the batch by `unfinished_index`
            # therefore, we don't need to filter the batch again in scheduler
            if len(new_indices) != len(self.topk_p):
                logger.warning(
                    "length of new_indices: %d != length of topk_p: %d, this should not happen",
                    len(new_indices),
                    len(self.topk_p),
                )
            self.topk_p = self.topk_p[: len(new_indices)]
            self.topk_index = self.topk_index[: len(new_indices)]
            self.hidden_states = self.hidden_states[: len(new_indices)]
            self.verified_id = self.verified_id[: len(new_indices)]
        else:
            # in some cases(e.g draft_extend), we have not filtered the batch by `unfinished_index`
            self.topk_p = self.topk_p[new_indices]
            self.topk_index = self.topk_index[new_indices]
            self.hidden_states = self.hidden_states[new_indices]
            self.verified_id = self.verified_id[new_indices]

    def merge_batch(self, spec_info: EagleDraftInput):
        # FIXME(pc) need support overlap here
        if self.hidden_states is None:
            self.hidden_states = spec_info.hidden_states
            self.verified_id = spec_info.verified_id
            self.topk_p = spec_info.topk_p
            self.topk_index = spec_info.topk_index
            return
        if spec_info.hidden_states is None:
            return
        # FIXME(pc) this operate should be put on cpu
        self.hidden_states = jnp.concatenate([self.hidden_states, spec_info.hidden_states], axis=0)
        self.verified_id = jnp.concatenate([self.verified_id, spec_info.verified_id], axis=0)
        self.topk_p = jnp.concatenate([self.topk_p, spec_info.topk_p])
        self.topk_index = jnp.concatenate([self.topk_index, spec_info.topk_index])


@dataclass
class EagleVerifyOutput:
    # Draft input batch
    draft_input: EagleDraftInput
    # Logit outputs from target worker
    logits_output: LogitsProcessorOutput
    # Accepted token ids including the bonus token
    verified_id: jax.Array
    # Accepted token length per sequence in a batch in CPU.
    accept_length_per_req_cpu: list[int]
    # Accepted indices from logits_output.next_token_logits
    accepted_indices: jax.Array


@register_pytree_node_class
@dataclass
class EagleVerifyInput:
    # container type for pytree
    draft_token: jax.Array
    custom_mask: jax.Array
    positions: jax.Array
    retrive_index: jax.Array
    retrive_next_token: jax.Array
    retrive_next_sibling: jax.Array
    retrive_cum_len: jax.Array
    seq_lens_cpu: jax.Array
    # common type for pytree
    spec_steps: int
    topk: int
    draft_token_num: int
    seq_lens_sum: int
    capture_hidden_mode: CaptureHiddenMode
    # grammar: BaseGrammarObject = None

    def tree_flatten(self):
        seq_lens_sum_arr = _as_int32_array(self.seq_lens_sum, fallback=0)

        children = (
            self.draft_token,
            self.custom_mask,
            self.positions,
            self.retrive_index,
            self.retrive_next_token,
            self.retrive_next_sibling,
            self.retrive_cum_len,
            self.seq_lens_cpu,
            seq_lens_sum_arr,
        )

        aux_data = {
            "spec_steps": self.spec_steps,
            "topk": self.topk,
            "draft_token_num": self.draft_token_num,
            "capture_hidden_mode": self.capture_hidden_mode,
        }
        return (children, aux_data)

    @classmethod
    def tree_unflatten(cls, aux_data, children):
        obj = cls.__new__(cls)
        obj.spec_steps = aux_data["spec_steps"]
        obj.topk = aux_data["topk"]
        obj.draft_token_num = aux_data["draft_token_num"]
        obj.capture_hidden_mode = aux_data["capture_hidden_mode"]

        obj.draft_token = children[0]
        obj.custom_mask = children[1]
        obj.positions = children[2]
        obj.retrive_index = children[3]
        obj.retrive_next_token = children[4]
        obj.retrive_next_sibling = children[5]
        obj.retrive_cum_len = children[6]
        obj.seq_lens_cpu = children[7]
        obj.seq_lens_sum = children[8]

        return obj

    def prepare_for_verify(
        self, model_worker_batch: ModelWorkerBatch, page_size: int, target_worker: ModelWorker
    ):
        if model_worker_batch.forward_mode.is_idle():
            return

        # TODO: keep draft_token on TPU
        # bs = len(model_worker_batch.req_pool_indices)
        model_worker_batch.input_ids = self.draft_token

        # bs = batch.batch_size()
        prefix_lens = model_worker_batch.seq_lens
        seq_lens_with_draft_token = model_worker_batch.seq_lens + self.draft_token_num
        # extend_lens = jnp.array([self.draft_token_num] * bs)
        model_worker_batch.return_hidden_states = False
        model_worker_batch.forward_mode = ForwardMode.TARGET_VERIFY
        model_worker_batch.spec_info = self
        model_worker_batch.capture_hidden_mode = CaptureHiddenMode.FULL

        # assert model_worker_batch.capture_hidden_mode == spec_info.capture_hidden_mode

    def sample(
        self,
        model_worker_batch: ModelWorkerBatch,
        logits_output: LogitsProcessorOutput,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        page_size: int,
        rng: nnx.Rngs,
        vocab_mask: jax.Array | None = None,  # For grammar
    ) -> jax.Array:
        """
        Verify and find accepted tokens based on logits output and batch
        (which contains spec decoding information).

        WARNING: This API in-place modifies the states of logits_output

        This API updates values inside logits_output based on the accepted
        tokens. I.e., logits_output.next_token_logits only contains
        accepted token logits.
        """
        if model_worker_batch.forward_mode.is_idle():
            return EagleVerifyOutput(
                draft_input=EagleDraftInput.create_idle_input(
                    hidden_size=model_worker_batch.model_config.hidden_size,
                    dtype=model_worker_batch.model_config.dtype,
                    topk=self.topk,
                    capture_hidden_mode=CaptureHiddenMode.LAST,
                ),
                logits_output=logits_output,
                verified_id=jnp.empty(0, dtype=jnp.int32),
                accept_length_per_req_cpu=[],
                accepted_indices=jnp.full(
                    (0, self.spec_steps + 1),
                    -1,
                    dtype=jnp.int32,
                ),
            )

        bs = self.retrive_index.shape[0]
        candidates = self.draft_token.reshape(bs, self.draft_token_num)
        sampling_info = model_worker_batch.sampling_info

        predict_shape = list(logits_output.next_token_logits.shape)[:-1]
        predict_shape[-1] += 1
        predict = jnp.empty(predict_shape, dtype=jnp.int32)

        accept_index = jnp.full((bs, self.spec_steps + 1), -1, dtype=jnp.int32)
        accept_length = jnp.empty((bs,), dtype=jnp.int32)

        if bs != len(sampling_info):
            sampling_info = copy.deepcopy(sampling_info)
            # NOTE: retrive_index are the indices of the requests that are kept.
            sampling_info.filter_batch(self.retrive_index.tolist(), self.retrive_index)

        # Sample tokens. Force greedy sampling on AMD
        is_all_greedy = sampling_info.is_all_greedy

        if is_all_greedy:
            target_predict = jnp.argmax(logits_output.next_token_logits, axis=-1).flatten()
            accept_index, accept_length, predict = verify_tree_greedy(
                predicts=predict,  # mutable
                accept_index=accept_index,  # mutable
                accept_token_num=accept_length,  # mutable
                candidates=candidates,
                retrive_index=self.retrive_index,
                retrive_next_token=self.retrive_next_token,
                retrive_next_sibling=self.retrive_next_sibling,
                target_predict=target_predict,
            )
        else:

            # apply temperature and get target probs
            expanded_temperature = jnp.repeat(
                sampling_info.temperatures, self.draft_token_num
            )  # (bs * draft_token_num, 1)
            expanded_temperature = jnp.expand_dims(expanded_temperature, axis=-1)
            target_probs = jax.nn.softmax(
                logits_output.next_token_logits / expanded_temperature, axis=-1
            )  # (bs * draft_token_num, vocab_size)
            target_probs = top_k_renorm_prob(
                target_probs, jnp.repeat(sampling_info.top_ks, self.draft_token_num)
            )

            if not jnp.all(sampling_info.top_ps == 1.0):
                target_probs = top_p_renorm_prob(
                    target_probs, jnp.repeat(sampling_info.top_ps, self.draft_token_num)
                )

            # TODO: optimize top_k and top_p by avoiding sort
            rngs = jax.random.split(rng.params(), 3)

            draft_probs = jnp.zeros(target_probs.shape, dtype=jnp.float32)

            # coins for rejection sampling
            coins = jax.random.uniform(rngs[1], candidates.shape, dtype=jnp.float32)
            # coins for final sampling
            coins_for_final_sampling = jax.random.uniform(rngs[2], (bs,), dtype=jnp.float32)
            accept_index, accept_length, predict = tree_speculative_sampling_target_only(
                predicts=predict,
                accept_index=accept_index,
                accept_token_num=accept_length,
                candidates=candidates,
                retrive_index=self.retrive_index,
                retrive_next_token=self.retrive_next_token,
                retrive_next_sibling=self.retrive_next_sibling,
                uniform_samples=coins,
                uniform_samples_for_final_sampling=coins_for_final_sampling,
                target_probs=target_probs,
                draft_probs=draft_probs,
                threshold_single=global_server_args_dict["speculative_accept_threshold_single"],
                threshold_acc=global_server_args_dict["speculative_accept_threshold_acc"],
                deterministic=True,
            )

        if SIMULATE_ACC_LEN:
            # Do simulation
            _, rng = jax.random.split(rng.params())
            accept_index, accept_length, predict = _generate_simulated_accept_index(
                accept_index=accept_index,
                predict=predict,
                accept_length=accept_length,
                simulate_acc_len=SIMULATE_ACC_LEN,
                bs=bs,
                spec_steps=self.spec_steps,
                rng=rng,
            )

        accept_length = accept_length + 1
        verified_id = predict[accept_index]
        return predict, verified_id, accept_length, accept_index


def _generate_simulated_accept_index(
    accept_index: jax.Array,
    predict,
    accept_length,
    simulate_acc_len,
    bs,
    spec_steps,
    rng: nnx.Rngs,
):
    simulate_acc_len_float = float(simulate_acc_len)
    if SIMULATE_ACC_METHOD == "multinomial":
        # here data is on cpu
        simulated_values = numpy.random.normal(
            loc=simulate_acc_len_float,
            scale=1.0,
            size=(1,),
        )
        # clamp simulated values to be between 1 and self.spec_steps
        simulated_values = jnp.clip(simulated_values, min=1.0, max=spec_steps + 1)
        simulate_acc_len = int(simulated_values.round().item())
    elif SIMULATE_ACC_METHOD == "match-expected":
        # multinomial sampling does not match the expected length
        # we keep it for the sake of compatibility of existing tests
        # but it's better to use "match-expected" for the cases that need to
        # match the expected length, One caveat is that this will only sample
        # either round down or round up of the expected length
        simulate_acc_len_float = max(1.0, min(spec_steps + 1, simulate_acc_len_float))
        lower = int(simulate_acc_len_float // 1)
        upper = lower + 1 if lower < spec_steps + 1 else lower
        if lower == upper:
            simulate_acc_len = lower
        else:
            weight_upper = simulate_acc_len_float - lower
            weight_lower = 1.0 - weight_upper
            # here, data is on cpu
            probs = jnp.array([weight_lower, weight_upper])
            sampled_index = jax.random.categorical(rng, jnp.log(probs))
            simulate_acc_len = lower if sampled_index == 0 else upper
    else:
        raise ValueError(f"Invalid simulate_acc_method: {SIMULATE_ACC_METHOD}")

    accept_indx_first_col = accept_index[:, 0].reshape(-1, 1)
    sim_accept_index = jnp.full((bs, spec_steps + 1), -1, dtype=jnp.int32)
    sim_accept_index = sim_accept_index.at[:, :simulate_acc_len].set(
        accept_indx_first_col + jnp.arange(simulate_acc_len)
    )
    accept_length = accept_length.at[:].set(simulate_acc_len - 1)
    predict = predict.at[:].set(100)  # some legit token id
    return sim_accept_index, accept_length, predict


def build_eagle_tree_structure(
    parent_list: jax.Array,
    selected_index: jax.Array,
    verified_seq_len: jax.Array,
    bs: int,
    draft_token_num: int,
    topk: int,
    depth: int,
    seq_lens_sum: int,
    tree_mask_mode: int = 0,  # FULL_MASK = 0
    max_seq_len_per_req: int | None = None,
    max_tree_mask_size: int | None = None,
    actual_tree_mask_size: int | None = None,
) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:
    """
    Args:
        parent_list: Parent indices array [bs, topk * (depth-1) + 1]
        selected_index: Selected token indices [bs, draft_token_num - 1]
        verified_seq_len: Sequence lengths [bs]
        bs: Batch size
        draft_token_num: Number of draft tokens (num_verify_tokens)
        topk: Top-k value
        depth: Tree depth
        seq_lens_sum: Sum of sequence lengths
        tree_mask_mode: Tree mask mode (0=FULL_MASK)
        max_seq_len_per_req: Static upper bound for sequence length per request
        max_tree_mask_size: Optional explicit capacity for the tree mask buffer
        actual_tree_mask_size: Optional override for the exact number of valid mask entries

    Returns:
        tuple of (tree_mask, positions, retrive_index, retrive_next_token, retrive_next_sibling)
    """

    if tree_mask_mode == 0:  # FULL_MASK
        inferred_actual_size = (
            seq_lens_sum * draft_token_num + draft_token_num * draft_token_num * bs
        )
        tree_mask_size = (
            inferred_actual_size if actual_tree_mask_size is None else actual_tree_mask_size
        )
        inferred_capacity = (
            max_seq_len_per_req * draft_token_num * bs + draft_token_num * draft_token_num * bs
            if max_seq_len_per_req is not None
            else inferred_actual_size
        )
        tree_mask_capacity = inferred_capacity if max_tree_mask_size is None else max_tree_mask_size
    else:
        inferred_actual_size = bs * draft_token_num * draft_token_num
        tree_mask_size = (
            inferred_actual_size if actual_tree_mask_size is None else actual_tree_mask_size
        )
        tree_mask_capacity = (
            inferred_actual_size if max_tree_mask_size is None else max_tree_mask_size
        )

    tree_mask = jnp.zeros((tree_mask_capacity,), dtype=jnp.bool_)
    if tree_mask_size > 0:
        tree_mask = tree_mask.at[:tree_mask_size].set(True)
    positions = jnp.zeros((bs * draft_token_num,), dtype=jnp.int32)
    retrive_index = jnp.full((bs, draft_token_num), -1, dtype=jnp.int32)
    retrive_next_token = jnp.full((bs, draft_token_num), -1, dtype=jnp.int32)
    retrive_next_sibling = jnp.full((bs, draft_token_num), -1, dtype=jnp.int32)

    for bid in range(bs):
        seq_len = verified_seq_len[bid]

        # Calculate seq_tree_idx for this batch (exactly like CUDA kernel)
        seq_tree_idx = draft_token_num * draft_token_num * bid
        if tree_mask_mode == 0:  # FULL_MASK
            for i in range(bid):
                seq_tree_idx += verified_seq_len[i] * draft_token_num
        for tid in range(draft_token_num):
            global_token_idx = bid * draft_token_num + tid

            # Calculate token_tree_idx for tree_mask
            if tree_mask_mode == 0:  # FULL_MASK
                token_tree_idx = seq_tree_idx + (seq_len + draft_token_num) * tid + seq_len + 1
            else:
                token_tree_idx = draft_token_num * draft_token_num * bid + draft_token_num * tid + 1

            # Set tree_mask for this token
            if token_tree_idx > 0 and token_tree_idx <= tree_mask_size:
                tree_mask = tree_mask.at[token_tree_idx - 1].set(True)

            # Clear next draft_token_num - 1 positions
            for i in range(draft_token_num - 1):
                mask_idx = token_tree_idx + i
                if mask_idx < tree_mask_size:
                    tree_mask = tree_mask.at[mask_idx].set(False)

            if tid == 0:
                # Verified token (tid == 0)
                positions = positions.at[global_token_idx].set(seq_len)
                retrive_index = retrive_index.at[bid, tid].set(global_token_idx)

                # Build retrive_next_token and retrive_next_sibling (backwards iteration)
                retrive_index_offset = bid * draft_token_num
                for i in range(draft_token_num - 1, 0, -1):  # i from draft_token_num-1 to 1
                    current_token_idx = retrive_index_offset + i
                    retrive_index = retrive_index.at[bid, i].set(current_token_idx)

                    selected_idx = bid * (draft_token_num - 1) + i - 1
                    parent_tb_idx = selected_index.flatten()[selected_idx] // topk
                    parent_position = 0

                    if parent_tb_idx > 0:
                        parent_list_idx = bid * (topk * (depth - 1) + 1) + parent_tb_idx
                        if parent_list_idx < parent_list.size:
                            parent_token_idx = parent_list.flatten()[parent_list_idx]

                            for parent_pos in range(draft_token_num - 1):
                                check_idx = bid * (draft_token_num - 1) + parent_pos
                                if (
                                    check_idx < selected_index.size
                                    and selected_index.flatten()[check_idx] == parent_token_idx
                                ):
                                    parent_position = parent_pos + 1  # +1 to convert to 1-indexed
                                    break
                            else:
                                parent_position = draft_token_num  # Not found
                        else:
                            parent_position = draft_token_num  # Invalid parent_list_idx
                    else:
                        parent_position = 0  # Root node

                    if parent_position >= draft_token_num:
                        # Invalid parent, skip
                        continue

                    next_token_idx = bid * draft_token_num + parent_position
                    if retrive_next_token.flatten()[next_token_idx] == -1:
                        retrive_next_token = retrive_next_token.at[bid, parent_position].set(i)
                    else:
                        # There's already a next_token, so set sibling
                        origin_next_token = retrive_next_token.flatten()[next_token_idx]
                        retrive_next_token = retrive_next_token.at[bid, parent_position].set(i)
                        retrive_next_sibling = retrive_next_sibling.at[bid, i].set(
                            origin_next_token
                        )

                retrive_index = retrive_index.at[bid, 0].set(bid * draft_token_num)

            else:
                # Draft token (tid > 0)
                # Calculate position by tracing back to root
                position = 0
                cur_position = tid - 1  # Convert to 0-indexed for selected_index

                while True:
                    position += 1
                    mask_idx = token_tree_idx + cur_position
                    if mask_idx < tree_mask_size:
                        tree_mask = tree_mask.at[mask_idx].set(True)

                    selected_idx = bid * (draft_token_num - 1) + cur_position
                    parent_tb_idx = selected_index.flatten()[selected_idx] // topk

                    if parent_tb_idx == 0:
                        # Reached root
                        break

                    parent_list_idx = bid * (topk * (depth - 1) + 1) + parent_tb_idx
                    if parent_list_idx < parent_list.size:
                        token_idx = parent_list.flatten()[parent_list_idx]

                        found = False
                        for cur_pos in range(draft_token_num - 1):
                            check_idx = bid * (draft_token_num - 1) + cur_pos
                            if (
                                check_idx < selected_index.size
                                and selected_index.flatten()[check_idx] == token_idx
                            ):
                                cur_position = cur_pos
                                found = True
                                break

                        if not found:
                            break  # Invalid tree structure
                    else:
                        break  # Invalid parent_list_idx

                positions = positions.at[global_token_idx].set(position + seq_len)
                retrive_index = retrive_index.at[bid, tid].set(global_token_idx)

    return tree_mask, positions, retrive_index, retrive_next_token, retrive_next_sibling


def get_src_tgt_cache_loc(
    seq_lens: jax.Array,
    out_cache_loc: jax.Array,
    accept_index: jax.Array,
    accept_length: jax.Array,
    draft_token_num: int,
    page_size: int,
):
    src_cache_loc = out_cache_loc[accept_index]
    extended_len = seq_lens + draft_token_num
    keep_len = jnp.minimum(
        (seq_lens + accept_length + 1 + page_size - 1) // page_size * page_size,
        extended_len,
    )
    to_free_num_slots = extended_len - keep_len
    return src_cache_loc, to_free_num_slots


def create_accept_length_filter(
    accept_length: jax.Array,
    unfinished_index_device: jax.Array,
    seq_lens: jax.Array,
):
    accept_length_filter = jnp.zeros_like(accept_length)
    accept_length_filter[unfinished_index_device] = accept_length[unfinished_index_device] + 1
    seq_lens.add_(accept_length + 1)
    return accept_length_filter


def assign_req_to_token_pool(
    req_pool_indices,
    req_to_token_pool,
    start_offsets,
    end_offsets,
    out_cache_loc,
):
    bs = start_offsets.shape[0]
    out_cache_lens = end_offsets - start_offsets
    out_cache_loc_start_positions = np.concatenate(
        [np.array([0], dtype=np.int32), np.cumsum(out_cache_lens)]
    )[0:-1]

    for i in range(bs):
        out_cache_loc_start = out_cache_loc_start_positions[i]
        req_to_token_pool.write(
            (req_pool_indices[i], slice(start_offsets[i], end_offsets[i])),
            out_cache_loc[out_cache_loc_start : out_cache_loc_start + out_cache_lens[i]],
        )

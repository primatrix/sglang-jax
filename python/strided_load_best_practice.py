"""
Pallas Kernel Strided Load æœ€ä½³å®žè·µ

æ€»ç»“ä¸‰ç§ä¸»è¦æ–¹æ¡ˆåŠå…¶é€‚ç”¨åœºæ™¯
"""

import jax
from jax import numpy as jnp
from jax.experimental import pallas as pl


# ============================================================================
# æ–¹æ¡ˆ1: ç›´æŽ¥ 2D åˆ‡ç‰‡ï¼ˆæŽ¨èç”¨äºŽè¡Œä¼˜å…ˆè¯­ä¹‰ï¼‰
# ============================================================================
def strided_load_direct_2d(kv_ref, o_ref, *, start, step, chunk_size=128):
    """
    æœ€ç®€å•é«˜æ•ˆçš„æ–¹æ¡ˆï¼šä¸éœ€è¦ reshapeï¼Œç›´æŽ¥åœ¨åŽŸå§‹ 2D æ•°æ®ä¸Šåš strided access

    ä¼˜ç‚¹ï¼š
    - âœ… ä¸éœ€è¦ reshape
    - âœ… ä»£ç æ¸…æ™°æ˜“æ‡‚
    - âœ… è¡Œä¼˜å…ˆè¯­ä¹‰ï¼ˆæ ‡å‡† NumPy/JAX è¡Œä¸ºï¼‰
    - âœ… æ— é¢å¤–æ•°æ®å¤åˆ¶

    é€‚ç”¨åœºæ™¯ï¼š
    - åŽŸå§‹æ•°æ®æ˜¯ 2Dï¼Œéœ€è¦è¡Œä¼˜å…ˆçš„ strided load
    - ä¸éœ€è¦åŽç»­åœ¨ (*,128) å½¢çŠ¶ä¸Šåšå…¶ä»–æ“ä½œ
    """
    r, l = kv_ref.shape
    folds = l // chunk_size
    output_rows = (r * folds - start + step - 1) // step

    for out_idx in range(output_rows):
        idx_1d = start + out_idx * step
        src_row = idx_1d // folds
        src_col_start = (idx_1d % folds) * chunk_size
        o_ref[out_idx, :] = kv_ref[src_row, src_col_start : src_col_start + chunk_size]


# ============================================================================
# æ–¹æ¡ˆ2: åˆ—ä¼˜å…ˆ reshape + strided loadï¼ˆflash_attention.py çš„æ–¹å¼ï¼‰
# ============================================================================
def strided_load_col_major_reshape(kv_ref, o_ref, *, start, step):
    """
    ä½¿ç”¨ reference reshapeï¼ˆåˆ—ä¼˜å…ˆï¼‰+ ç›´æŽ¥ strided indexing

    ä¼˜ç‚¹ï¼š
    - âœ… Zero-cost reshapeï¼ˆä»…å†…å­˜è§†å›¾å˜æ¢ï¼‰
    - âœ… ä»£ç ç®€æ´

    ç¼ºç‚¹ï¼š
    - âŒ åˆ—ä¼˜å…ˆè¯­ä¹‰ï¼ˆä¸åŒäºŽæ ‡å‡† NumPy/JAXï¼‰

    é€‚ç”¨åœºæ™¯ï¼š
    - æ•°æ®æœ¬èº«æ˜¯äº¤é”™æŽ’åˆ—çš„ï¼ˆå¦‚ Flash Attention çš„ KV cacheï¼‰
    - åˆ—ä¼˜å…ˆè¯­ä¹‰æ­£å¥½ç¬¦åˆä½ çš„éœ€æ±‚
    """
    r, l = kv_ref.shape
    folds = l // 128
    reshaped = kv_ref.reshape(r * folds, 128)
    o_ref[...] = reshaped[start::step]


# ============================================================================
# æ–¹æ¡ˆ3: åˆ—ä¼˜å…ˆ reshape + ç´¢å¼•æ˜ å°„ï¼ˆå…¼é¡¾ä¸¤ç§è¯­ä¹‰ï¼‰
# ============================================================================
def strided_load_with_index_mapping(kv_ref, o_ref, *, start, step, chunk_size=128):
    """
    å…ˆåšåˆ—ä¼˜å…ˆ reshapeï¼Œç„¶åŽé€šè¿‡ç´¢å¼•æ˜ å°„å®žçŽ°è¡Œä¼˜å…ˆè¯­ä¹‰

    ä¼˜ç‚¹ï¼š
    - âœ… Zero-cost reshape
    - âœ… è¡Œä¼˜å…ˆè¯­ä¹‰
    - âœ… reshape åŽçš„æ•°æ®å¯ç”¨äºŽå…¶ä»–æ“ä½œ

    ç¼ºç‚¹ï¼š
    - âŒ éœ€è¦ç´¢å¼•è½¬æ¢è®¡ç®—

    é€‚ç”¨åœºæ™¯ï¼š
    - éœ€è¦ reshape åŽçš„ (*,128) å½¢çŠ¶ç”¨äºŽåŽç»­æ“ä½œ
    - åŒæ—¶æƒ³è¦è¡Œä¼˜å…ˆçš„ strided load è¯­ä¹‰
    """
    r, l = kv_ref.shape
    folds = l // chunk_size

    # åˆ—ä¼˜å…ˆ reshape
    reshaped = kv_ref.reshape(r * folds, chunk_size)
    output_rows = (r * folds - start + step - 1) // step

    for out_idx in range(output_rows):
        row_major_idx = start + out_idx * step

        # ç´¢å¼•è½¬æ¢ï¼šè¡Œä¼˜å…ˆ -> åˆ—ä¼˜å…ˆ
        src_i = row_major_idx // folds
        src_j = row_major_idx % folds
        col_major_idx = src_j * r + src_i

        if col_major_idx < r * folds:
            o_ref[out_idx, :] = reshaped[col_major_idx, :]


# ============================================================================
# ç¤ºä¾‹å’Œæ€§èƒ½å¯¹æ¯”
# ============================================================================
if __name__ == "__main__":
    print("=" * 80)
    print("Pallas Kernel Strided Load æœ€ä½³å®žè·µ")
    print("=" * 80)

    # æµ‹è¯•æ•°æ®
    data = jnp.array(
        [
            jnp.arange(256) + 0,
            jnp.arange(256) + 1000,
            jnp.arange(256) + 2000,
            jnp.arange(256) + 3000,
        ],
        dtype=jnp.float32,
    )

    start, step = 1, 2
    output_size = (8 - start + step - 1) // step

    print("\næµ‹è¯•æ•°æ®: (4, 256)")
    print(f"Strided load å‚æ•°: start={start}, step={step}")
    print(f"é¢„æœŸè¾“å‡º: {output_size} è¡Œ")

    # æ ‡å‡†å‚è€ƒ
    expected = data.reshape(8, 128)[start::step]
    print(f"\næ ‡å‡†è¡Œä¼˜å…ˆç»“æžœ:")
    print(f"  ç¬¬0è¡Œ: [{expected[0,0]:.0f}...{expected[0,-1]:.0f}]")
    print(f"  ç¬¬1è¡Œ: [{expected[1,0]:.0f}...{expected[1,-1]:.0f}]")

    # æ–¹æ¡ˆ1: ç›´æŽ¥ 2D åˆ‡ç‰‡
    print("\n" + "=" * 80)
    print("æ–¹æ¡ˆ1: ç›´æŽ¥ 2D åˆ‡ç‰‡ï¼ˆæŽ¨èï¼‰")
    print("=" * 80)
    result1 = pl.pallas_call(
        lambda kv, o: strided_load_direct_2d(kv, o, start=start, step=step),
        out_shape=jax.ShapeDtypeStruct((output_size, 128), jnp.float32),
    )(data)
    print(f"  ç»“æžœåŒ¹é…: {jnp.allclose(result1, expected)}")
    print(f"  ç¬¬0è¡Œ: [{result1[0,0]:.0f}...{result1[0,-1]:.0f}]")
    print("\n  ðŸ’¡ æŽ¨èåœºæ™¯: é»˜è®¤é€‰æ‹©ï¼Œç®€å•ç›´æŽ¥")

    # æ–¹æ¡ˆ2: åˆ—ä¼˜å…ˆ reshape
    print("\n" + "=" * 80)
    print("æ–¹æ¡ˆ2: åˆ—ä¼˜å…ˆ reshapeï¼ˆflash_attention.py æ–¹å¼ï¼‰")
    print("=" * 80)
    result2 = pl.pallas_call(
        lambda kv, o: strided_load_col_major_reshape(kv, o, start=start, step=step),
        out_shape=jax.ShapeDtypeStruct((output_size, 128), jnp.float32),
    )(data)
    print(f"  ç»“æžœåŒ¹é…: {jnp.allclose(result2, expected)}")
    print(f"  ç¬¬0è¡Œ: [{result2[0,0]:.0f}...{result2[0,-1]:.0f}]")
    if not jnp.allclose(result2, expected):
        print("\n  âš ï¸  æ³¨æ„: è¿™æ˜¯åˆ—ä¼˜å…ˆè¯­ä¹‰ï¼Œä¸Žæ ‡å‡†è¡Œä¼˜å…ˆä¸åŒ")
        print("  ðŸ’¡ æŽ¨èåœºæ™¯: æ•°æ®æœ¬èº«æ˜¯äº¤é”™æŽ’åˆ—ï¼ˆå¦‚ KV cacheï¼‰")

    # æ–¹æ¡ˆ3: ç´¢å¼•æ˜ å°„
    print("\n" + "=" * 80)
    print("æ–¹æ¡ˆ3: åˆ—ä¼˜å…ˆ reshape + ç´¢å¼•æ˜ å°„")
    print("=" * 80)
    result3 = pl.pallas_call(
        lambda kv, o: strided_load_with_index_mapping(kv, o, start=start, step=step),
        out_shape=jax.ShapeDtypeStruct((output_size, 128), jnp.float32),
    )(data)
    print(f"  ç»“æžœåŒ¹é…: {jnp.allclose(result3, expected)}")
    print(f"  ç¬¬0è¡Œ: [{result3[0,0]:.0f}...{result3[0,-1]:.0f}]")
    print("\n  ðŸ’¡ æŽ¨èåœºæ™¯: éœ€è¦ reshape åŽçš„æ•°æ®åšå…¶ä»–æ“ä½œ")

    # æ€»ç»“
    print("\n" + "=" * 80)
    print("é€‰æ‹©å»ºè®®")
    print("=" * 80)
    print(
        """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ éœ€æ±‚                                    æŽ¨èæ–¹æ¡ˆ                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é»˜è®¤é€‰æ‹©                               æ–¹æ¡ˆ1ï¼ˆç›´æŽ¥ 2D åˆ‡ç‰‡ï¼‰     â”‚
â”‚ è¡Œä¼˜å…ˆ + ç®€å•                          æ–¹æ¡ˆ1ï¼ˆç›´æŽ¥ 2D åˆ‡ç‰‡ï¼‰     â”‚
â”‚ åˆ—ä¼˜å…ˆ + äº¤é”™æ•°æ®                      æ–¹æ¡ˆ2ï¼ˆåˆ—ä¼˜å…ˆ reshapeï¼‰   â”‚
â”‚ éœ€è¦ (*,128) å½¢çŠ¶ + è¡Œä¼˜å…ˆè¯­ä¹‰          æ–¹æ¡ˆ3ï¼ˆç´¢å¼•æ˜ å°„ï¼‰        â”‚
â”‚ Flash Attention å¼çš„ KV cache          æ–¹æ¡ˆ2ï¼ˆåˆ—ä¼˜å…ˆ reshapeï¼‰   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ ¸å¿ƒè¦ç‚¹ï¼š
1. æ–¹æ¡ˆ1 æœ€ç®€å•é«˜æ•ˆï¼Œé€‚åˆå¤§éƒ¨åˆ†åœºæ™¯
2. æ–¹æ¡ˆ2 ç”¨äºŽ zero-cost reshapeï¼Œä½†æ³¨æ„åˆ—ä¼˜å…ˆè¯­ä¹‰
3. æ–¹æ¡ˆ3 ç”¨äºŽéœ€è¦ reshape åŽå½¢çŠ¶çš„é«˜çº§åœºæ™¯
    """
    )
